model_hparams:
  module_name: # add you model module name, e.g., `my_module.MyModel`

  # add your model's parameters as specified in its `ModelHparams`

  optimizer:
    module_name: torch.optim.Adam
    kwargs:
      lr: 0.001
      # add further kwargs for your optimizer

  scheduler:
    # e.g.
    module_name: torch.optim.lr_scheduler.ExponentialLR
    kwargs:
      gamma: 0.999
      # add further kwargs for your scheduler

  checkpoint:
    monitor: validation/loss  # monitors the validation loss for checkpointing
    save_last: true  # always checkpoints the latest run
    save_top_k: 5  # saves the best 5 runs
    save_every_n_epochs: 1  # evaluate checkpoints every epoch

run_hparams:
  trainer:
    accelerator: cpu  # use gpu
    max_epochs: 1_000  # number of epochs to train
    precision: 64  # model weight precision (should be the same as dataset precision)

  logger:
    project: TestProject  # name of the project in your w&b profile
    log_model: false  # whether to log the model to w&b
    offline: false  # whether to upload the logs to w&b

data_hparams:
  module_name:  # add you dataset module name, e.g. my_module.MyDataset
  loader:
    batch_size: 64  # batch size used in your dataloader
    shuffle: False  # whether to shuffle the dataset
    pin_memory: False  # whether to pin memory (allows faster data transfer to the GPU)
    drop_last: False  # whether to drop the last incomplete batch
    num_workers: 0  # how many subprocesses to use loading the data (0 = main process)
    persistent_workers: False  # whether to terminate the worker processes after loading the data
  split: [ 0.8, 0.1, 0.1 ]  # dataset split into train, validation (and test)
  seed: 42  # Seed to use to split the dataset

hydra:
  job:
    name: TestRun  # name of your run
  run:
    dir: lightning_logs/${hydra.job.name}/${now:%Y-%m-%d_%H-%M-%S}
  sweep:
    dir: lightning_logs/${hydra.job.name}
    subdir: ${hydra.job.override_dirname}
